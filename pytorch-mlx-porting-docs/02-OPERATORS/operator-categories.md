# Operator Categories

## Purpose

PyTorch's 2,666 operators are organized into logical categories using a tag-based system. This categorization helps:
- Understand operator functionality at a high level
- Prioritize which operators to port first
- Group similar operations together for batch implementation
- Track compilation and optimization requirements

This document explains PyTorch's categorization system, provides comprehensive category listings, and establishes a tiered approach for MLX porting.

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────┐
│                   Operator Categorization                    │
└─────────────────────────────────────────────────────────────┘
                            │
        ┌───────────────────┼───────────────────┐
        ▼                   ▼                   ▼
   ┌─────────┐        ┌─────────┐        ┌──────────┐
   │  Tags   │        │Function │        │ Dispatch │
   │ System  │        │  Type   │        │  Backend │
   └─────────┘        └─────────┘        └──────────┘
        │                   │                   │
        │                   │                   │
   tags.yaml          Mathematical         CPU/CUDA/MPS
   (20 tags)          Categories           Availability
                      (8 groups)
```

## Tag System

PyTorch uses **tags** defined in `tags.yaml` to categorize operators by their properties, not just their mathematical function. Each operator can have multiple tags.

### Core Tags

**File**: `/Users/zakkeown/Code/flashlight/reference/pytorch/aten/src/ATen/native/tags.yaml`

#### Functional Tags

**`core` (Critical for Export/Compilation)**
- **Purpose**: Operators that remain after aten-to-aten decomposition and functionalization
- **Properties**: Fully functional, SSA-compliant, no inplace/out variants
- **Count**: ~300 operators
- **Usage**: Export, torch.compile, compiler backends
- **Example**: `add.Tensor`, `mul.Scalar`, `matmul`, `relu`

```yaml
# From tags.yaml:69-79
- tag: core
  desc: |
    Core aten ops is a subset of aten ops that remains after aten-to-aten
    decomposition and functionalization pass. This opset is designed to serve
    as the functional IR to interface with compiler backends.
```

**`pointwise` (Element-wise Operations)**
- **Purpose**: Each output element computed only from corresponding input elements
- **Properties**: Broadcasting applied, independent element computation
- **Count**: ~200 operators
- **Examples**: `add`, `mul`, `div`, `relu`, `sigmoid`, `exp`, `log`
- **Performance**: Highly parallelizable, memory-bound

```yaml
# From tags.yaml:80-84
- tag: pointwise
  desc: |
    Pointwise operators are operators where each element of the output is
    computed only by accessing the corresponding element of all the
    broadcasted inputs.
```

**`reduction` (Aggregation Operations)**
- **Purpose**: Compute aggregate values across dimensions
- **Properties**: Reduces tensor rank, computes summary statistics
- **Count**: ~50 operators
- **Examples**: `sum`, `mean`, `max`, `min`, `argmax`, `var`, `std`
- **Performance**: Compute-intensive, requires careful parallelization

```yaml
# From tags.yaml:96-100
- tag: reduction
  desc: |
    This tag indicates that an operator performs a reduction operation,
    computing aggregate values (sum, mean, max, min, etc.) across one or
    more dimensions of the input tensor(s).
```

#### Memory and Aliasing Tags

**`inplace_view` (Metadata-only Operations)**
- **Purpose**: Only modifies tensor metadata, not data
- **Examples**: `transpose_`, `squeeze_`, `unsqueeze_`
- **MLX Note**: MLX uses similar zero-copy view semantics

**`view_copy` (Explicit Copy Variants)**
- **Purpose**: Copy variants of view operators (e.g., `reshape_copy`)
- **Pattern**: Named `{op}_copy` where `{op}` is a view operator
- **Usage**: Functionalization pass converts views to copies

**`generated` (Auto-generated Operators)**
- **Purpose**: Not explicitly in YAML, generated by codegen
- **Examples**: Out variants, some backward ops

#### Determinism and Randomness Tags

**`nondeterministic_seeded` (Controlled Randomness)**
- **Purpose**: Random operations controlled by Generator
- **Examples**: `rand`, `randn`, `randint`, `dropout`
- **Property**: Reproducible with same seed
- **MLX Note**: MLX uses key-based PRNG (different paradigm)

**`nondeterministic_bitwise` (Non-reproducible)**
- **Purpose**: Cannot guarantee bitwise reproducibility across runs
- **Examples**: Some CUDA reductions, atomicAdd-based operations
- **Reason**: Floating-point non-associativity, parallel execution order

#### Data Dependency Tags

**`dynamic_output_shape` (Data-dependent Shapes)**
- **Purpose**: Output shape depends on input data values
- **Examples**: `unique`, `nonzero`, `masked_select`
- **Compilation**: Challenging for static compilers
- **MLX Note**: Requires dynamic allocation

**`data_dependent_output` (Data-dependent Non-Tensor Outputs)**
- **Purpose**: Non-tensor outputs depend on tensor data
- **Examples**: `item()`, `tolist()`, size queries with data
- **Compilation**: Cannot run with meta tensors

#### Memory Layout Tags

**`needs_exact_strides` (Strict Stride Requirements)**
- **Purpose**: Operator requires exact strides from eager mode
- **Usage**: Inductor compilation hints
- **Priority**: Most restrictive

**`needs_contiguous_strides` (Contiguity Required)**
- **Purpose**: Tensors must be contiguous
- **Usage**: Many low-level kernels
- **MLX Note**: MLX generally handles arbitrary strides

**`needs_fixed_stride_order` (Fixed Permutation)**
- **Purpose**: Stride permutation must match eager
- **Priority**: Less restrictive than exact strides

**`flexible_layout` (No Layout Requirements)**
- **Purpose**: Can accept varying strides/storage_offset
- **Usage**: Allows Inductor to optimize layout
- **Priority**: Least restrictive

#### Compilation Tags

**`pt2_compliant_tag` (PyTorch 2.x Compatible)**
- **Purpose**: Guaranteed to work with torch.compile, torch.export
- **Validation**: Tested with opcheck
- **Coverage**: Expanding set

**`cudagraph_unsafe` (CUDAGraph Incompatible)**
- **Purpose**: Cannot be captured in CUDA graphs
- **Examples**: Dynamic allocations, CPU synchronization
- **Effect**: Inductor splits graph around these ops

**`maybe_aliasing_or_mutating` (Statically Ambiguous)**
- **Purpose**: Cannot determine if functional at compile time
- **Usage**: Decompose before functionalization
- **Export**: Unsafe to preserve

## Mathematical Categories

Beyond tags, operators naturally group by mathematical function. These categories are not explicitly defined in PyTorch but are useful for understanding and porting.

### 1. Arithmetic Operations

**Element-wise Binary Operations**
- **Operators**: `add`, `sub`, `mul`, `div`, `remainder`, `fmod`, `pow`
- **Variants**: Tensor-Tensor, Tensor-Scalar
- **Tags**: `[core, pointwise]`
- **Count**: ~30 operators (including variants)
- **MLX Equivalent**: Direct mapping to mlx operators

```cpp
// Example from native_functions.yaml:554
- func: add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
  structured_delegate: add.out
  variants: function, method
  tags: [core, pointwise]
```

**Element-wise Unary Operations**
- **Operators**: `neg`, `abs`, `sqrt`, `exp`, `log`, `sin`, `cos`, `tan`
- **Tags**: `[core, pointwise]` or `[pointwise]`
- **Count**: ~40 operators
- **MLX Equivalent**: Direct mapping

**Comparison Operations**
- **Operators**: `eq`, `ne`, `lt`, `gt`, `le`, `ge`
- **Return Type**: Bool tensor
- **Tags**: `[core, pointwise]`
- **Count**: ~15 operators

### 2. Linear Algebra

**Matrix Multiplication**
- **Operators**: `mm`, `bmm`, `matmul`, `addmm`, `baddbmm`
- **Tags**: Often `[core]` for basic variants
- **Count**: ~20 operators
- **Performance**: Compute-intensive, optimized via BLAS/cuBLAS

```cpp
// Example from native_functions.yaml:3841
- func: matmul(Tensor self, Tensor other) -> Tensor
  variants: function, method
  dispatch:
    CompositeImplicitAutograd: matmul
```

**Vector Operations**
- **Operators**: `mv`, `dot`, `vdot`, `outer`
- **Tags**: Varies
- **Count**: ~10 operators

**Advanced Linear Algebra (linalg namespace)**
- **Operators**: `linalg_norm`, `linalg_qr`, `linalg_svd`, `linalg_eig`, `linalg_inv`
- **Count**: ~60 operators
- **MLX Note**: MLX has `linalg` module with similar operations

### 3. Reduction Operations

**Basic Reductions**
- **Operators**: `sum`, `mean`, `prod`, `max`, `min`
- **Variants**: All dims, specific dims, keepdim
- **Tags**: `[core, reduction]` or `[reduction]`
- **Count**: ~30 operators

```cpp
// Example from native_functions.yaml:5946
- func: sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
  structured_delegate: sum.IntList_out
  tags: [core, reduction]
```

**Statistical Reductions**
- **Operators**: `var`, `std`, `median`, `quantile`, `nanmean`, `nansum`
- **Tags**: `[reduction]`
- **Count**: ~20 operators

**Argmin/Argmax**
- **Operators**: `argmin`, `argmax`, `topk`, `kthvalue`
- **Return**: Index tensors
- **Tags**: `[reduction]`
- **Count**: ~10 operators

### 4. Shape Manipulation

**View Operations (Zero-copy)**
- **Operators**: `view`, `reshape`, `transpose`, `permute`, `squeeze`, `unsqueeze`, `flatten`
- **Tags**: `[core]` for many, `[inplace_view]` for inplace variants
- **Count**: ~40 operators
- **MLX Note**: MLX uses similar zero-copy semantics

**Copy Operations**
- **Operators**: `clone`, `contiguous`, `{op}_copy` variants
- **Tags**: `[view_copy]` for copy variants
- **Count**: ~20 operators

**Size-changing Operations**
- **Operators**: `expand`, `repeat`, `tile`, `broadcast_to`
- **Tags**: Varies
- **Count**: ~15 operators

### 5. Indexing and Selection

**Indexing Operations**
- **Operators**: `index`, `index_select`, `gather`, `take`, `masked_select`
- **Tags**: `[core]` for some
- **Count**: ~30 operators

```cpp
// Example from native_functions.yaml:9570
- func: gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor
  variants: method, function
  structured_delegate: gather.out
  tags: core
```

**Scatter Operations**
- **Operators**: `scatter`, `scatter_add`, `scatter_reduce`, `index_put`
- **Tags**: Varies
- **Count**: ~20 operators

**Slicing**
- **Operators**: `slice`, `select`, `narrow`, `split`, `chunk`
- **Tags**: `[core]` for some
- **Count**: ~15 operators

### 6. Convolution and Pooling

**Convolution**
- **Operators**: `conv1d`, `conv2d`, `conv3d`, `conv_transpose1d/2d/3d`
- **Tags**: Generally not tagged `core` (decomposable)
- **Count**: ~15 operators
- **MLX Note**: MLX has similar conv operations

```cpp
// Example from native_functions.yaml:1757
- func: conv2d(Tensor input, Tensor weight, Tensor? bias=None,
               SymInt[2] stride=1, SymInt[2] padding=0,
               SymInt[2] dilation=1, SymInt groups=1) -> Tensor
  dispatch:
    CompositeImplicitAutograd: conv2d_symint
```

**Pooling**
- **Operators**: `max_pool1d/2d/3d`, `avg_pool1d/2d/3d`, `adaptive_avg_pool`, `adaptive_max_pool`
- **Count**: ~25 operators

**Upsampling**
- **Operators**: `upsample_nearest`, `upsample_bilinear`, `interpolate`
- **Count**: ~15 operators

### 7. Activation Functions

**Standard Activations**
- **Operators**: `relu`, `gelu`, `sigmoid`, `tanh`, `softmax`, `log_softmax`, `silu`, `mish`
- **Tags**: `[core, pointwise]` for most
- **Count**: ~30 operators

```cpp
// Example from native_functions.yaml:5181
- func: relu(Tensor self) -> Tensor
  variants: function, method
  dispatch:
    CPU, CUDA: relu
    MPS: relu_mps
  tags: [core, pointwise]
```

**Advanced Activations**
- **Operators**: `elu`, `leaky_relu`, `prelu`, `glu`, `softplus`, `hardsigmoid`
- **Tags**: `[pointwise]` typically
- **Count**: ~20 operators

### 8. Normalization

**Batch/Layer Normalization**
- **Operators**: `batch_norm`, `layer_norm`, `group_norm`, `instance_norm`
- **Count**: ~15 operators
- **MLX Note**: MLX has similar normalization operations

**Other Normalization**
- **Operators**: `normalize`, `local_response_norm`
- **Count**: ~5 operators

### 9. Random Number Generation

**Distributions**
- **Operators**: `rand`, `randn`, `randint`, `rand_like`, `randn_like`
- **Tags**: `[nondeterministic_seeded]`
- **Count**: ~20 operators
- **MLX Note**: MLX uses key-based PRNG (different API)

**Random Sampling**
- **Operators**: `multinomial`, `bernoulli`, `normal`, `uniform`, `poisson`
- **Tags**: `[nondeterministic_seeded]`
- **Count**: ~15 operators

### 10. Loss Functions

**Standard Losses**
- **Operators**: `nll_loss`, `cross_entropy`, `binary_cross_entropy`, `mse_loss`, `l1_loss`
- **Count**: ~20 operators
- **MLX Note**: Often implemented in Python layer

### 11. Sparse Operations

**Sparse Tensor Operations**
- **Operators**: Operators with `SparseCPU`, `SparseCUDA` dispatch keys
- **Count**: ~100 operators have sparse variants
- **MLX Note**: MLX sparse support is limited

### 12. Special Functions

**Mathematical Special Functions**
- **Operators**: `erf`, `erfc`, `lgamma`, `digamma`, `polygamma`, `i0`, `i1`
- **Count**: ~30 operators
- **Tags**: `[pointwise]` typically

## Priority Tiers for MLX Porting

Given 2,666 total operators, we use a tiered approach focusing on core ML workload support.

### Tier 1: Essential Core (~50 operators)

**Priority**: CRITICAL - Needed for basic tensor operations and simple models

**Arithmetic (12)**
- `add.Tensor`, `add.Scalar`, `sub.Tensor`, `sub.Scalar`
- `mul.Tensor`, `mul.Scalar`, `div.Tensor`, `div.Scalar`
- `neg`, `abs`, `pow.Tensor_Scalar`, `sqrt`

**Linear Algebra (8)**
- `mm`, `bmm`, `matmul`, `mv`, `dot`
- `addmm`, `baddbmm`, `tensordot`

**Activations (7)**
- `relu`, `gelu`, `sigmoid`, `tanh`, `softmax`, `log_softmax`, `silu`

**Reductions (8)**
- `sum`, `mean`, `max`, `min`, `argmax`, `argmin`, `prod`, `var`

**Shape Operations (10)**
- `reshape`, `view`, `transpose`, `permute`, `squeeze`, `unsqueeze`
- `expand`, `flatten`, `cat`, `stack`

**Indexing (5)**
- `gather`, `scatter`, `index_select`, `masked_fill`, `where`

**Convolutions (3)**
- `conv1d`, `conv2d`, `conv3d`

**Pooling (2)**
- `max_pool2d`, `avg_pool2d`

**Criterion**: Tagged with `[core]` and used in >80% of models

### Tier 2: Extended Support (~200 operators)

**Priority**: HIGH - Needed for production models

**Categories**:
- All Tier 1 variants (inplace, out, backward)
- Additional activations (elu, leaky_relu, prelu, etc.)
- All shape manipulation (split, chunk, narrow, etc.)
- Statistical ops (std, median, quantile)
- Comparison ops (eq, ne, lt, gt, le, ge)
- Logical ops (and, or, not, xor)
- Element-wise math (exp, log, sin, cos, tan, etc.)
- Additional convolution/pooling variants
- Normalization (batch_norm, layer_norm, group_norm)
- Embedding operations

**Implementation Strategy**: Pattern-based from Tier 1

### Tier 3: Comprehensive Coverage (~2,400 operators)

**Priority**: MEDIUM - Long-tail operations

**Categories**:
- Sparse operations
- Advanced linear algebra (SVD, eigendecomposition, etc.)
- Special functions
- Image operations (grid_sample, affine_grid, etc.)
- RNN/LSTM primitives
- Distributed operations
- Quantization operations
- FFT operations
- Advanced indexing variants

**Implementation Strategy**: As-needed during actual porting

### Operator Distribution by Tags

```
Tag Distribution (approximate counts):

core:              ~300 operators  [***************************]
pointwise:         ~200 operators  [******************]
reduction:         ~50 operators   [*****]
view_copy:         ~30 operators   [***]
nondeterministic_seeded: ~35 ops  [***]
dynamic_output_shape: ~15 ops     [**]
pt2_compliant_tag: ~500 operators [**************] (growing)
```

### Backend Availability

```
Backend Support (approximate):

CPU:               ~2,400 operators [**********************]
CUDA:              ~2,200 operators [********************]
MPS:               ~800 operators   [********]
Meta:              ~1,500 operators [*************]
SparseCPU/CUDA:    ~150 operators   [**]
Quantized:         ~100 operators   [*]
```

**MLX Implication**: Target MPS operator count (~800) as initial goal

## Tag-Based Selection Strategy

### Finding Core Operators

```bash
# Parse YAML to extract core operators
grep -A 1 "tags:.*core" native_functions.yaml | \
  grep "^- func:" | \
  cut -d':' -f2 | \
  cut -d'(' -f1 | \
  sort -u
```

### Finding Pointwise Operators

```bash
# Extract pointwise operators
grep -A 1 "tags:.*pointwise" native_functions.yaml | \
  grep "^- func:" | \
  cut -d':' -f2 | \
  cut -d'(' -f1 | \
  sort -u
```

### Finding Reduction Operators

```bash
# Extract reduction operators
grep -A 1 "tags:.*reduction" native_functions.yaml | \
  grep "^- func:" | \
  cut -d':' -f2 | \
  cut -d'(' -f1 | \
  sort -u
```

## Operator Count by Variant

```
Variant Distribution:

function only:     ~800 operators
method only:       ~200 operators
function+method:   ~1,400 operators (700 ops × 2 variants)
out variants:      ~600 operators
inplace variants:  ~400 operators
backward ops:      ~500 operators
```

**Total unique operations**: ~1,000 (excluding variants)
**Total operator entries**: ~2,666 (including all variants)

## MLX Porting Considerations

### Tag Mapping to MLX

**`core` Tag → MLX Priority 1**
- These operators are essential for graph export and compilation
- Should be first implemented in MLX
- Approximately 300 operators

**`pointwise` Tag → Highly Parallelizable**
- MLX's Metal backend excels at element-wise operations
- Can leverage Metal's parallel compute capabilities
- Easy to vectorize

**`reduction` Tag → Careful Metal Implementation**
- Requires careful reduction kernel design
- Metal has shared memory for efficient reductions
- Consider using Metal Performance Shaders (MPS) where available

**`nondeterministic_seeded` → MLX Random Number Generation**
- MLX uses key-based PRNG (like JAX)
- Different API paradigm from PyTorch's Generator
- Requires wrapper layer for compatibility

### Backend Priority for Reference

1. **MPS Backend**: Most relevant reference (Metal-based)
   - `/Users/zakkeown/Code/flashlight/reference/pytorch/aten/src/ATen/mps/`
   - `/Users/zakkeown/Code/flashlight/reference/pytorch/aten/src/ATen/native/mps/`
   - ~800 operators implemented

2. **CPU Backend**: Fallback reference
   - `/Users/zakkeown/Code/flashlight/reference/pytorch/aten/src/ATen/native/cpu/`
   - Simple, readable implementations

3. **CUDA Backend**: Algorithm reference (adapt to Metal)
   - `/Users/zakkeown/Code/flashlight/reference/pytorch/aten/src/ATen/native/cuda/`
   - Parallel patterns transferable to Metal

### Implementation Strategy

**Phase 1**: Implement all Tier 1 `[core]` operators
- Essential for basic models
- ~50 operators
- Full compatibility testing

**Phase 2**: Add Tier 2 tagged operators
- `[core, pointwise]`: Element-wise ops
- `[core, reduction]`: Aggregations
- ~150 additional operators

**Phase 3**: Fill out Tier 2 completeness
- Shape operations
- Advanced activations
- Normalization
- ~200 total Tier 2 operators

**Phase 4**: Long-tail Tier 3 (as-needed)
- Sparse operations (if needed)
- Special functions
- Advanced linalg
- ~2,400 operators (implement on demand)

### Testing Strategy

**Tag-based Test Suites**:
```python
# Test all core operators
core_ops = get_ops_with_tag("core")
for op in core_ops:
    test_mlx_pytorch_equivalence(op)

# Test all pointwise operators
pointwise_ops = get_ops_with_tag("pointwise")
for op in pointwise_ops:
    test_broadcasting(op)
    test_metal_vectorization(op)
```

### Automatic Operator Catalog Generation

For Tier 3 operators, generate catalog from YAML:

```python
# Parse native_functions.yaml
import yaml

with open("native_functions.yaml") as f:
    ops = yaml.safe_load(f)

# Generate markdown table
for op in ops:
    func = op["func"]
    tags = op.get("tags", [])
    dispatch = op.get("dispatch", {})

    print(f"| {func} | {tags} | {list(dispatch.keys())} |")
```

## Critical Files for Category Analysis

**Tag Definitions**:
- `/Users/zakkeown/Code/flashlight/reference/pytorch/aten/src/ATen/native/tags.yaml`

**Operator Definitions**:
- `/Users/zakkeown/Code/flashlight/reference/pytorch/aten/src/ATen/native/native_functions.yaml`

**Code Generation** (uses tags):
- `/Users/zakkeown/Code/flashlight/reference/pytorch/torchgen/model.py`
- `/Users/zakkeown/Code/flashlight/reference/pytorch/torchgen/api/translate.py`

**MPS Backend** (shows Metal implementation priorities):
- `/Users/zakkeown/Code/flashlight/reference/pytorch/aten/src/ATen/native/mps/operations/` (operator implementations)

## Summary

PyTorch's 2,666 operators are systematically organized through:

1. **Tag System**: 20 tags categorize by properties (core, pointwise, reduction, etc.)
2. **Mathematical Groups**: 12 functional categories (arithmetic, linalg, convolution, etc.)
3. **Variant Explosion**: ~1,000 unique operations with 2-3 variants each

**MLX Porting Strategy**:
- **Tier 1** (~50 ops): Core operations for basic models
- **Tier 2** (~200 ops): Production model support
- **Tier 3** (~2,400 ops): Long-tail, implement as-needed

**Priority Tags**:
- `[core]`: Export/compile essential (~300 ops)
- `[core, pointwise]`: Element-wise critical ops
- `[core, reduction]`: Aggregation critical ops

**Reference Priority**:
1. MPS backend (~800 ops) - Metal-based, most similar to MLX
2. CPU backend - Simple reference implementations
3. CUDA backend - Parallel algorithm patterns

This tiered, tag-based approach allows systematic porting without overwhelming scope.
