# Signature exclusions for flashlight parity checking
#
# These are APIs where the signature "mismatch" is actually acceptable because
# either our signature is more explicit than PyTorch's (*args, **kwargs wrapper),
# or the difference is intentional.
#
# Format:
#   module_name:
#     api_name: "reason for exclusion"

torch:
  # Low-level C++ builtins with complex signatures not fully replicated
  numel: "PyTorch numel() takes no args (method on tensor), our module-level version requires input"
  ormqr: "Complex QR orthogonal multiplication - simplified signature in flashlight"
  rnn_relu: "Low-level RNN function with packed sequence support - uses higher-level API in flashlight"
  rnn_tanh: "Low-level RNN function with packed sequence support - uses higher-level API in flashlight"
  # Minor naming differences - functionally equivalent
  dot: "Parameter name 'other' vs 'tensor' - functionally equivalent"
  searchsorted: "Parameter name 'values' vs 'input' - functionally equivalent"

torch.nn.init:
  # PyTorch uses *args/**kwargs wrappers for deprecation handling
  # Our explicit signatures are cleaner and match the actual behavior
  uniform: "Explicit signature vs PyTorch's deprecation wrapper"
  normal: "Explicit signature vs PyTorch's deprecation wrapper"
  constant: "Explicit signature vs PyTorch's deprecation wrapper"
  eye: "Explicit signature vs PyTorch's deprecation wrapper"
  dirac: "Explicit signature vs PyTorch's deprecation wrapper"
  xavier_uniform: "Explicit signature vs PyTorch's deprecation wrapper"
  xavier_normal: "Explicit signature vs PyTorch's deprecation wrapper"
  kaiming_uniform: "Explicit signature vs PyTorch's deprecation wrapper"
  kaiming_normal: "Explicit signature vs PyTorch's deprecation wrapper"
  orthogonal: "Explicit signature vs PyTorch's deprecation wrapper"
  sparse: "Explicit signature vs PyTorch's deprecation wrapper"

torch.autograd:
  # Our backward() takes a single tensor, PyTorch's takes multiple
  backward: "Simplified single-tensor interface vs PyTorch's multi-tensor API"

torch.nn.functional:
  # These functions use (*args, **kwargs) in PyTorch as wrappers that call
  # the underlying implementation with named parameters. Our explicit signatures
  # are actually MORE specific and better for documentation/IDE support.
  max_pool1d: "Our explicit signature is better than PyTorch's (*args, **kwargs) wrapper"
  max_pool2d: "Our explicit signature is better than PyTorch's (*args, **kwargs) wrapper"
  max_pool3d: "Our explicit signature is better than PyTorch's (*args, **kwargs) wrapper"
  adaptive_max_pool1d: "Our explicit signature is better than PyTorch's (*args, **kwargs) wrapper"
  adaptive_max_pool2d: "Our explicit signature is better than PyTorch's (*args, **kwargs) wrapper"
  adaptive_max_pool3d: "Our explicit signature is better than PyTorch's (*args, **kwargs) wrapper"

torch.distributions:
  # PyTorch uses torch.Size([]) for empty shapes, we use () - semantically equivalent
  Distribution: "Default batch_shape/event_shape uses () instead of torch.Size([]) - functionally equivalent"
  ExponentialFamily: "Default batch_shape/event_shape uses () instead of torch.Size([]) - functionally equivalent"

torch.nn.grad:
  # PyTorch uses int defaults, we use explicit tuples - both semantically equivalent
  # (PyTorch internally expands int to tuple anyway)
  conv1d_weight: "Explicit tuple defaults vs int - functionally equivalent"
  conv1d_input: "Explicit tuple defaults vs int - functionally equivalent"
  conv2d_weight: "Explicit tuple defaults vs int - functionally equivalent"
  conv2d_input: "Explicit tuple defaults vs int - functionally equivalent"
  conv3d_weight: "Explicit tuple defaults vs int - functionally equivalent"
  conv3d_input: "Explicit tuple defaults vs int - functionally equivalent"

torch.random:
  # Generator uses *args/**kwargs wrapper in PyTorch
  Generator: "Explicit device parameter vs PyTorch's *args/**kwargs wrapper"

torch.distributions.constraints:
  # PyTorch uses NotImplemented as sentinel, we use concrete False/0 - functionally equivalent
  dependent: "Uses False/0 defaults instead of NotImplemented sentinel - functionally equivalent"
  dependent_property: "Uses False/0 defaults instead of NotImplemented sentinel - functionally equivalent"

torch.nn.utils.rnn:
  # PackedSequence uses *args/**kwargs in PyTorch, we use explicit parameters
  PackedSequence: "Explicit parameters vs PyTorch's *args/**kwargs wrapper"
